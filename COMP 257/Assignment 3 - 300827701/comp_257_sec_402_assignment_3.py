# -*- coding: utf-8 -*-
"""COMP 257 SEC 402 - Assignment 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UIsztZzj2426qh187UKY8ArLKO8RKmUd

##Assignment 1
Name: Jonathan Au <br>
Student #: 300827701 <br>

##Question 1
This assignment will be similar to Assignment 2 but we will use hierarchical clustering in place of K-Means.
1.	Retrieve and load the Olivetti faces dataset [0 points]

2.	Split the training set, a validation set, and a test set using stratified sampling to ensure that there are the same number of images per person in each set. [0 points]

3.	Using k-fold cross validation, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. [0 points]

4.	Using either Agglomerative Hierarchical Clustering (AHC) or Divisive Hierarchical Clustering (DHC) and using the centroid-based clustering rule, reduce the dimensionality of the set by using the following similarity measures:

- a) Euclidean Distance [20 points]

- b) Minkowski Distance [20 points]

- c) Cosine Similarity [20 points]

5.	Discuss any discrepancies observed between 4(a), 4(b), or 4(c).
Use the silhouette score approach to choose the number of clusters for 4(a), 4(b), and 4(c). [10 points]

6. Use the set from (4(a), 4(b), or 4(c)) to train a classifier as in (3) using k-fold cross validation. [30 points]
"""

#Imports
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.datasets import fetch_olivetti_faces
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, KFold

import warnings
warnings.filterwarnings("ignore")

"""1.	Retrieve and load the Olivetti faces dataset [0 points]


"""

# Load olivetti faces
olivetti = fetch_olivetti_faces()
X = olivetti.data
Y = olivetti.target
Z = olivetti.images

# Print the description and shapes of the dataset
#print(olivetti.DESCR)
print(X.shape)
print(Y.shape)
print(Z.shape)

"""2.	Split the training set, a validation set, and a test set using stratified sampling to ensure that there are the same number of images per person in each set. [0 points]


"""

#split into train and test data
X_train, X_val_test, y_train, y_val_test = train_test_split(X, Y, test_size=.665)
#X_train, X_val_test, y_train, y_val_test = train_test_split(X, Y, test_size=.4)
#A test size of .665 was given such that the train data would be slightly larger as it was impossible for all three of them to be completly equal

#split train data into train and validation data
X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5)

#print the shapes of the data
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

"""3.	Using k-fold cross validation, train a classifier to predict which person is represented in each picture, and evaluate it on the validation set. [0 points]"""

#Use K-fold for cross val
kFoldCV = KFold(n_splits=5)
kFoldCV.get_n_splits(X)
print(kFoldCV)

#Training classifier for prediction
#y_pred = cross_val_predict(kmean, X_train, y_train, cv=kFoldCV)
y_pred = cross_val_predict(LogisticRegression(), X_train, y_train, cv=kFoldCV)
#y_pred = cross_val_predict(RandomForestRegressor(), X_train, y_train, cv=kFoldCV)
print("Y Prediction: ", y_pred)

score = cross_val_score(LogisticRegression(), X_train, y_train, cv=kFoldCV)
#score = cross_val_score(RandomForestRegressor(), X_train, y_train, cv=kFoldCV)
print("Score: ", score)

"""4.	Using either Agglomerative Hierarchical Clustering (AHC) or Divisive Hierarchical Clustering (DHC) and using the centroid-based clustering rule, reduce the dimensionality of the set by using the following similarity measures:

- a) Euclidean Distance [20 points]

- b) Minkowski Distance [20 points]

- c) Cosine Similarity [20 points]

5.	Discuss any discrepancies observed between 4(a), 4(b), or 4(c).
Use the silhouette score approach to choose the number of clusters for 4(a), 4(b), and 4(c). [10 points]


"""

range_n_clusters = [2, 3, 4, 5, 6]
dist_measure = ['euclidean', 'minkowski', 'cosine']
for i in dist_measure:
  if i == 'minkowski':
    link='average'
  elif i == 'cosine':
    link='complete'
  else:
    link='ward'

  ahc_clf = AgglomerativeClustering(n_clusters=3, metric=i, linkage=link)
  ahc_clf.fit(X)
  data_lbl = ahc_clf.labels_
  print("Data Labels: ", data_lbl)

  #Print Silhouette Score
  print(i, " Silhouette Score: ", silhouette_score(X, data_lbl))
  for n_clusters in range_n_clusters:
    #Use Silhouette score to choose cluster numbers
    clusterer = KMeans(n_clusters=n_clusters)
    cluster_labels = clusterer.fit_predict(X)
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg)

  #Plot data
  plt.scatter(X[:, 0], X[:, 1], c=data_lbl, cmap='viridis')
  plt.xlabel('X Feature')
  plt.ylabel('Y Feature')
  plt.title(f'Heirarchical Clustering: {i}')
  plt.show()

  #Plot Dendrogram
  plt.figure(figsize=(20,7))
  plt.xlabel('Data Labels')
  plt.ylabel('Distance')
  plt.title(f'Heirarchical Clustering Dendrogram: {i}')
  dendrogrm = sch.dendrogram(sch.linkage(X, link, metric=i),
                             labels=data_lbl,
                             leaf_rotation=90,
                             leaf_font_size=8,
                             show_contracted=True)
  plt.show()

"""6. Use the set from (4(a), 4(b), or 4(c)) to train a classifier as in (3) using k-fold cross validation. [30 points]"""

#Selected Silhouette score Cluster
ahc_clf = AgglomerativeClustering(n_clusters=4, metric='euclidean', linkage='ward')
ahc_clf.fit(X)
data_lbl = ahc_clf.labels_
#print("Data Labels: ", data_lbl)

#Print Silhouette Score
print(" Silhouette Score: ", silhouette_score(X, data_lbl))

#KFold Cross val
kFoldCV = KFold(n_splits=4)
kFoldCV.get_n_splits(X)
print(kFoldCV)
y_pred = cross_val_predict(RandomForestRegressor(), X_train, y_train, cv=kFoldCV)
print("Y Prediction: ", y_pred)

score = cross_val_score(RandomForestRegressor(), X_train, y_train, cv=kFoldCV)
print("Score: ", score)

"""7. A 5-minute video presentation is required as part of the submission to document the steps taken to obtain the results. Any submission without a demo video will result in a 75% penalty."""