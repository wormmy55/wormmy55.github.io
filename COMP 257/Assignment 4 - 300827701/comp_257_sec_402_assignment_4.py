# -*- coding: utf-8 -*-
"""COMP 257 SEC 402 - Assignment 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O5BRtiCXR06JTN3s9g5Tj0qWuCX3XYAr

##Assignment 4
Name: Jonathan Au <br>
Student #: 300827701 <br>

##Question 1
<b>In this assignment, you train a Gaussian mixture model on the Olivetti faces dataset.</b>
1.	Retrieve and load the Olivetti faces dataset. [0 points]
2.	Split the dataset into training, validation, and test sets using stratified sampling to ensure that each set contains the same number of images per person. [0 points]
3.	Apply PCA on the training data, preserving 99% of the variance, to reduce the dataset's dimensionality. [10 points]
4.	Determine the most suitable covariance type for the dataset. [15 points]
5.	Determine the minimum number of clusters that best represent the dataset using either AIC or BIC. [15 points]
6.	Plot the results from steps 3 and 4. [15 points]
7.	Output the hard clustering assignments for each instance to identify which cluster each image belongs to. [2.5 points]
8.	Output the soft clustering probabilities for each instance to show the likelihood of each image belonging to each cluster. [2.5 points]
9.	Use the model to generate some new faces (using the sample() method) and visualize them (use the inverse_transform() method to transform the data back to its original space based on the PCA method used). [15 points]
10.	Modify some images (e.g., rotate, flip, darken). [15 points]

Determine if the model can detect the anomalies produced in step 10 by comparing the output of the score_samples() method for normal images and for anomalies. [10 points]
"""

#Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch

from numpy.linalg import eig
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_olivetti_faces
from sklearn.cluster import AgglomerativeClustering
from sklearn.model_selection import StratifiedShuffleSplit

import warnings
warnings.filterwarnings("ignore")

"""1.	Retrieve and load the Olivetti faces dataset. [0 points]
2.	Split the dataset into training, validation, and test sets using stratified sampling to ensure that each set contains the same number of images per person. [0 points]

"""

# Load olivetti faces
olivetti = fetch_olivetti_faces()
#X = olivetti.data
#Y = olivetti.target
#Z = olivetti.images

X = pd.DataFrame(olivetti["data"])
Y = pd.DataFrame(olivetti["target"])

# Print the description and shapes of the dataset
#print(olivetti.DESCR)
print(X.shape)
print(Y.shape)
#print(Z.shape)

#split into train and test data
#X_train, X_val_test, y_train, y_val_test = train_test_split(X, Y, test_size=.665)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.665)
for i, (train_index, test_index) in enumerate(sss.split(X, Y)):
    X_train = X.iloc[train_index]
    y_train = Y.iloc[train_index]
    X_val_test = X.iloc[test_index]
    y_val_test = Y.iloc[test_index]

#split train data into train and validation data
#X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5)
sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5)
for i, (test_index, val_index) in enumerate(sss2.split(X_val_test, y_val_test)):
    X_test = X.iloc[test_index]
    y_test = Y.iloc[test_index]
    X_val = X.iloc[val_index]
    y_val = Y.iloc[val_index]

#print the shapes of the data
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

"""3.	Apply PCA on the training data, preserving 99% of the variance, to reduce the dataset's dimensionality. [10 points]

"""

# Create PCA instance
# set PCA to get 99% of variance
pca = PCA(n_components=0.99)

# Fit train data
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
pc = pca.fit_transform(X_train_scaled)

X_train_pca = pd.DataFrame(pca.fit_transform(X_train))


# Print the number of components and the explained variance ratio
print ( "Components =", pca.n_components_ , "\nTotal explained variance =",
      round(pca.explained_variance_ratio_.sum(),5)  )

"""4.	Determine the most suitable covariance type for the dataset. [15 points]

"""

cov_matrix = np.cov(X_train_pca.T)
eigenvalues, eigenvectors = eig(cov_matrix)
print("eigenvalues: \n", eigenvalues)
print("eigenvectors: \n", eigenvectors)

"""5.	Determine the minimum number of clusters that best represent the dataset using either AIC or BIC. [15 points]

"""

gmm = GaussianMixture().fit(X_train_pca)
print("Gaussian Mixture BIC: ", gmm.bic(X_train_pca))
print("Gaussian Mixture AIC: ", gmm.aic(X_train_pca))

print("Gaussian Mixture Weights: ", gmm.weights_)
print("Gaussian Mixture Means: \n", gmm.means_)
print("Gaussian Mixture Covariances: \n", gmm.covariances_)

#Plot BIC and AIC
n_components = np.arange(1, 21)
models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(pc)
          for n in n_components]

plt.plot(n_components, [m.bic(pc) for m in models], label='BIC')
plt.plot(n_components, [m.aic(pc) for m in models], label='AIC')
plt.legend(loc='best')
plt.xlabel('n_components');

"""The minimum number of clusters ranges from around 3 to 5

6.	Plot the results from steps 3 and 4. [15 points]
"""

#Plot Results
plt.scatter(X_train_pca.iloc[:, 0], X_train_pca.iloc[:, 1], c=y_train)
plt.xlabel("1st Principal Component")
plt.ylabel("2nd Principal Component")
#plt.legend()
plt.colorbar()
plt.title("PCA Projection")
plt.show()

plt.imshow(np.array(X_train.iloc[0]).reshape(64,64), cmap="gray")

"""7.	Output the hard clustering assignments for each instance to identify which cluster each image belongs to. [2.5 points]

8.	Output the soft clustering probabilities for each instance to show the likelihood of each image belonging to each cluster. [2.5 points]
"""

# Heirarchical Clustering
# 4 Clusters was selected as the minimum
ag_clf = AgglomerativeClustering(n_clusters=4, metric="euclidean", linkage="ward")
ag_clf.fit(X)
clf_labels = ag_clf.labels_
print("Data Labels: \n", clf_labels)
print("Silhouette Score: ", silhouette_score(X, clf_labels))

plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=clf_labels, s=40, cmap='viridis')
plt.xlabel("1st Feature")
plt.ylabel("2nd Feature")
plt.title("Heirarchical Clustering")
plt.show()

# Dendrogram
plt.figure(figsize=(20, 7))
plt.title("Agglomerative Clustering Dendrogram", fontsize=14)
plt.xlabel("Number of points in node")
plt.ylabel("Linkage distance")
dendro = sch.dendrogram(sch.linkage(X, "ward", metric="euclidean"),
                        labels = clf_labels,
                        leaf_rotation=90,
                        leaf_font_size=10,
                        show_contracted=True)
plt.show()

"""9.	Use the model to generate some new faces (using the sample() method) and visualize them (use the inverse_transform() method to transform the data back to its original space based on the PCA method used). [15 points]

"""

# Generating Faces
original_cluster_centers = ss.inverse_transform(pca.inverse_transform(pc))
print(original_cluster_centers)

plt.imshow(np.array(original_cluster_centers[0]).reshape(64,64))
plt.show()

#plt.title("Original")
#plt.imshow(np.array(X_train.iloc[0]).reshape(64,64), cmap="gray")
#plt.show()

#plt.title("Reconstructed with PCA")
#plt.imshow(np.array(X_train_norm.iloc[0]).reshape(64,64), cmap="gray")
#plt.show()

# generating New faces with Sample()
X_train_norm = pd.DataFrame(pca.inverse_transform(X_train_pca))

for i in range(10):
  plt.title(f"New Face {i} with Sample()")
  plt.imshow(np.array(X_train_norm.sample().iloc[0]).reshape(64,64), cmap="gray")
  plt.show()

"""10.	Modify some images (e.g., rotate, flip, darken). [15 points]"""

#No problems were detected, so this section is left empty

"""Determine if the model can detect the anomalies produced in step 10 by comparing the output of the score_samples() method for normal images and for anomalies. [10 points]"""