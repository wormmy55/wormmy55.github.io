{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment 6 Reinfircement Learning\n",
        "Name: Jonathan Au <br>\n",
        "Student #: 300827701 <br>"
      ],
      "metadata": {
        "id": "1gmUqHBnl_wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a DQN Agent to Successfully Land the Lunar Lander in OpenAI Gym’s LunarLander-v2 Environment\n",
        "1.\tCreate a simple DQN epsilon policy network with 4 output neurons (one per possible action).  <br>[Hint: DQN Agents use Epsilon greedy policy]        [15 points] <br>\n",
        "2.\tDiscuss the rationale of the activation functions & the loss function used in the network. [10 points]<br>\n",
        "3.\tDefine the hyperparameters: <br>\n",
        "  (i) the number of iterations, <br>\n",
        "  (ii) the number of episodes, <br>\n",
        "  (iii) the maximum number of steps, and <br>\n",
        "  (iv) the discount factor γ at each step. [50 points]<br>\n",
        "4.\tTrain the agent on the LunarLander-v2 environment for a sufficient number of episodes to achieve a satisfactory level of performance. [10 points]<br>\n",
        "5.\tAnalyze the agent's learning progress by plotting relevant performance metrics (e.g., cumulative rewards, episode length) over time. [10 points]<br>\n",
        "\n",
        "Discuss the challenges faced during training and potential strategies for further improving the agent's performance. [5 points]\n"
      ],
      "metadata": {
        "id": "uiXKoRsAl_pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy tensorflow gym\n",
        "!pip install Box2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GtV7Z6qk6ZbF",
        "outputId": "7cc5e73a-d3f0-4c0f-8392-bf87b5cd8cf3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: Box2D in /usr/local/lib/python3.10/dist-packages (2.3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OMo-D76sl_EI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#All imports\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "from numpy.linalg import eig\n",
        "from collections import deque\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from tensorflow.keras.models import load_model, Model, Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Dense, Input\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split, KFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tCreate a simple DQN epsilon policy network with 4 output neurons (one per possible action).  <br>[Hint: DQN Agents use Epsilon greedy policy] <br>[15 points]"
      ],
      "metadata": {
        "id": "UbImiBuVmrHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "input_shape = [4]  # == env.observation_space.shape\n",
        "n_outputs = 4  # == env.action_space.n\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape = input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "  if np.random.rand() <= epsilon:\n",
        "    return random.randrange(n_outputs)  # random action\n",
        "  else:\n",
        "    Q_values = model.predict(state.reshape((1, state_size)), verbose=0)\n",
        "    return np.argmax(Q_values[0])  # optimal action according to the DQN\n"
      ],
      "metadata": {
        "id": "9QP1UAfHmre-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Deep Q-Network\n",
        "def build_model():\n",
        "  model = Sequential([\n",
        "    Dense(24, input_dim=state_size, activation='relu'),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(action_size, activation='linear')\n",
        "  ])\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
        "  return model\n",
        "\n",
        "model = build_model()\n",
        "target_model = build_model()\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "# Function to update target network\n",
        "def update_target_model():\n",
        "  target_model.set_weights(model.get_weights())\n",
        "\n",
        "# Function to select action\n",
        "def select_action(state, epsilon):\n",
        "  if np.random.rand() <= epsilon:\n",
        "    return random.randrange(action_size)  # Explore\n",
        "  q_values = model.predict(state, verbose=0)\n",
        "  return np.argmax(q_values[0])  # Exploit\n"
      ],
      "metadata": {
        "id": "-1JAPTUub1Su",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tDiscuss the rationale of the activation functions & the loss function used in the network. <br>[10 points]"
      ],
      "metadata": {
        "id": "M77oV1J8mru9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not much rationale there. I just recycled what was used in the course code."
      ],
      "metadata": {
        "id": "H1s82mCXTv-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\tDefine the hyperparameters: <br>\n",
        "  (i) the number of iterations, <br>\n",
        "  (ii) the number of episodes, <br>\n",
        "  (iii) the maximum number of steps, and <br>\n",
        "  (iv) the discount factor γ at each step. <br>[50 points]"
      ],
      "metadata": {
        "id": "s9-5UzuKmsMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "#env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "\n",
        "# Initialize variables\n",
        "obs = env.reset(seed=42)\n",
        "batch_size = 64\n",
        "max_episodes = 500\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.01\n",
        "memory_size = 2000\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "#n_outputs = action_size\n",
        "\n",
        "\n",
        "replay_buffer = deque(maxlen = memory_size)\n",
        "\n",
        "alpha0 = 0.05 #initial learning rate\n",
        "decay = 0.005 #learning rate decay\n",
        "gamma = 0.90 #discount factor\n",
        "state = 0 # initial state\n",
        "discount_factor = 0.95\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, max_size):\n",
        "    self.buffer = np.empty(max_size, dtype=object)\n",
        "    self.max_size = max_size\n",
        "    self.index = 0\n",
        "    self.size = 0\n",
        "\n",
        "  def append(self, obj):\n",
        "    self.buffer[self.index] = obj\n",
        "    self.size = min(self.size + 1, self.max_size)\n",
        "    self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    indices = np.random.randint(self.size, size=batch_size)\n",
        "    return self.buffer[indices]\n",
        "\n",
        "\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(5)\n",
        "    ]  # [states, actions, rewards, next_states, dones, truncateds]\n",
        "\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    replay_buffer.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    runs = 1.0 - (dones)  # episode is not done or truncated\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "for episode in range(max_episodes):\n",
        "  obs = env.reset()\n",
        "  #obs = obs[0].reshape(1, state_size)\n",
        "  for step in range(200):\n",
        "    epsilon = max(1 - episode / 500, epsilon_min)\n",
        "    obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "    #obs = obs.reshape(1, state_size)\n",
        "    if done:\n",
        "      break\n",
        "  if episode > 50:\n",
        "    training_step(batch_size)"
      ],
      "metadata": {
        "id": "3nCgoJUdmsjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tTrain the agent on the LunarLander-v2 environment for a sufficient number of episodes to achieve a satisfactory level of performance. <br>[10 points]"
      ],
      "metadata": {
        "id": "vCqpFykBmvYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def replay():\n",
        "  if len(memory) < batch_size:\n",
        "    return\n",
        "\n",
        "  mini_batch = random.sample(memory, batch_size)\n",
        "  for state, action, reward, next_state, done in mini_batch:\n",
        "    target = reward\n",
        "    if not done:\n",
        "      target += gamma * np.amax(target_model.predict(next_state, verbose=0)[0])\n",
        "\n",
        "    target_q_values = model.predict(state, verbose=0)\n",
        "    target_q_values[0][action] = target\n",
        "    model.fit(state, target_q_values, epochs=1, verbose=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "3Ur7ABramvvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tAnalyze the agent's learning progress by plotting relevant performance metrics (e.g., cumulative rewards, episode length) over time. <br>[10 points]"
      ],
      "metadata": {
        "id": "DTb3tU0gmv7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "plt.plot(rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Deep Q-Learning Performance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kCy48uatmwXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss the challenges faced during training and potential strategies for further improving the agent's performance. <br>[5 points]"
      ],
      "metadata": {
        "id": "ETrhQShlnI4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't know because It took too long to run the hyperparameters"
      ],
      "metadata": {
        "id": "GO6ga_qdEEBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extra code\n",
        "  \"\"\"# Training loop\n",
        "rewards = []\n",
        "for episode in range(max_episodes):\n",
        "  state = env.reset()\n",
        "  state = np.reshape(state, [1, state_size])\n",
        "  total_reward = 0\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = select_action(state, epsilon)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "    memory.append((state, action, reward, next_state, done))\n",
        "    state = next_state\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "      update_target_model()\n",
        "      print(f\"Episode: {episode + 1}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "      break\n",
        "\n",
        "  rewards.append(total_reward)\n",
        "  replay()\n",
        "\n",
        "  if epsilon > epsilon_min:\n",
        "    epsilon *= epsilon_decay\n",
        "\n",
        "def show_one_episode(policy, n_max_steps=200, seed=42):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    np.random.seed(seed)\n",
        "    obs, info = env.reset(seed=seed)\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render())\n",
        "        action = policy(obs)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        if done or truncated:\n",
        "            break\n",
        "    env.close()\n",
        "    return plot_animation(frames)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "collapsed": true,
        "id": "S4I0aVdH536X",
        "outputId": "00492226-5dee-43cb-b892-012dabd1ae8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Training loop\\nrewards = []\\nfor episode in range(max_episodes):\\nstate = env.reset()\\nstate = np.reshape(state, [1, state_size])\\ntotal_reward = 0\\ndone = False\\n\\nwhile not done:\\n  action = select_action(state, epsilon)\\n  next_state, reward, done, _ = env.step(action)\\n  next_state = np.reshape(next_state, [1, state_size])\\n\\n  memory.append((state, action, reward, next_state, done))\\n  state = next_state\\n  total_reward += reward\\n\\n  if done:\\n    update_target_model()\\n    print(f\"Episode: {episode + 1}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\\n    break\\n\\nrewards.append(total_reward)\\nreplay()\\n\\nif epsilon > epsilon_min:\\n  epsilon *= epsilon_decay\\n\\ndef show_one_episode(policy, n_max_steps=200, seed=42):\\n  frames = []\\n  env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\\n  np.random.seed(seed)\\n  obs, info = env.reset(seed=seed)\\n  for step in range(n_max_steps):\\n      frames.append(env.render())\\n      action = policy(obs)\\n      obs, reward, done, truncated, info = env.step(action)\\n      if done or truncated:\\n          break\\n  env.close()\\n  return plot_animation(frames)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}